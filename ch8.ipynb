{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章: ニューラルネット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70.単語ベクトルの和による特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configs\n",
    "TRAIN_DATA_PATH ='./ch6_folder/save_data/train.csv'\n",
    "VALID_DATA_PATH = './ch6_folder/save_data/valid.csv'\n",
    "TEST_DATA_PATH = './ch6_folder/save_data/test.csv'\n",
    "GOOGLE_MODEL_PATH = './ch7_folder/model/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(GOOGLE_MODEL_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoshidatomoya/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model_vocab_list = list(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "記事の見出しの単語列 $(w_{i,1}, \\ldots, w_{i,T_i})$ に対して、embeddingをおこない、以下のような特徴ベクトルを生成すると書いてあります。\n",
    "\n",
    "$$\n",
    "    {x}_i = \\frac{1}{T_i}\\sum_{1 \\leq t \\leq T_i} {emb}(w_{i,t}) \n",
    "$$\n",
    "\n",
    "これは、各単語数に関する平均を特徴量にするということですね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_TAG = {'b': 0, 't': 1, 'e': 2, 'm': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "df_valid = pd.read_csv(VALID_DATA_PATH)\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titles_list = [title.split() for title in df_train['TITLE'].values.tolist()]\n",
    "valid_titles_list = [title.split() for title in df_valid['TITLE'].values.tolist()]\n",
    "test_titles_list = [title.split() for title in df_test['TITLE'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10672 1334 1334\n"
     ]
    }
   ],
   "source": [
    "train_length = len(train_titles_list)\n",
    "valid_length = len(valid_titles_list)\n",
    "test_length = len(test_titles_list)\n",
    "print(train_length, valid_length, test_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word: str) -> str:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    word = word.replace('-', '')\n",
    "    word = word.replace('.', '')\n",
    "    word = word.replace('\\'', '')\n",
    "    word = word.replace(':', '')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(titles_list):\n",
    "    feature_vecs_list = []\n",
    "    \n",
    "    for title in titles_list:\n",
    "        tmp_vecs_list =[]\n",
    "        for word in title:\n",
    "            word = tokenize(word)\n",
    "            try:\n",
    "                vec = model[word]\n",
    "                tmp_vecs_list.append(vec)\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        \n",
    "        if len(tmp_vecs_list) == 1:\n",
    "            feature_vec = tmp_vecs_list[0]\n",
    "        else:\n",
    "            np_vecs = np.array(tmp_vecs_list)\n",
    "            feature_vec = np.mean(np_vecs, axis=0)\n",
    "        feature_vecs_list.append(feature_vec)\n",
    "    return np.array(feature_vecs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_embedding_matrix(train_titles_list)\n",
    "X_valid = get_embedding_matrix(valid_titles_list)\n",
    "X_test = get_embedding_matrix(test_titles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10672, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1334, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1334, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= pd.read_csv(TRAIN_DATA_PATH)['CATEGORY'].map(CATEGORY_TAG).values\n",
    "y_valid = pd.read_csv(VALID_DATA_PATH)['CATEGORY'].map(CATEGORY_TAG).values\n",
    "y_test = pd.read_csv(TEST_DATA_PATH)['CATEGORY'].map(CATEGORY_TAG).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.eye(4)[y_train]\n",
    "Y_valid = np.eye(4)[y_valid]\n",
    "Y_test = np.eye(4)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10672,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10672, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1334,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1334, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1334,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1334, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. 単層NNによる予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "個人的にこういう時にKerasを使うのが好きなのでKerasを使っていきます。\n",
    "重みの初期化のところ、毎回同じ値を取りたいのでrandom seedみたいなことをしたいのですが、どうすればいいですかね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn():\n",
    "    inputs = Input(shape=(300,))\n",
    "    predicts = Dense(4, activation='softmax') (inputs)\n",
    "    model = Model(inputs=inputs, outputs=predicts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = X_train[:1]\n",
    "X_1_4 = X_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = predict_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 1204      \n",
      "=================================================================\n",
      "Total params: 1,204\n",
      "Trainable params: 1,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = model.predict(x_1)\n",
    "Y_1_4 = model.predict(X_1_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23747101, 0.26101652, 0.25859946, 0.24291295]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23747101, 0.26101652, 0.25859946, 0.24291295],\n",
       "       [0.2698821 , 0.22152211, 0.22567463, 0.28292125],\n",
       "       [0.23746988, 0.26058245, 0.23853041, 0.26341733],\n",
       "       [0.26514286, 0.24195269, 0.25889996, 0.23400448]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_1_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. 損失と勾配の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文面からは、実際自分でクロスエントロピー損失関数や、勾配を`numpy`や`math`を用いて作成して、作成した関数を用いて求めてほしいのか、71とかで使ったモジュールを用いてやって欲しいのかイマイチわからないのですが、自作はせずにkerasを使ってやります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.backend import gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = Y_train[:1]\n",
    "Y_train_1_4 = Y_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Computes the crossentropy loss between the labels and predictions.\n",
       "\n",
       "Use this crossentropy loss function when there are two or more label classes.\n",
       "We expect labels to be provided in a `one_hot` representation. If you want to\n",
       "provide labels as integers, please use `SparseCategoricalCrossentropy` loss.\n",
       "There should be `# classes` floating point values per feature.\n",
       "\n",
       "In the snippet below, there is `# classes` floating pointing values per\n",
       "example. The shape of both `y_pred` and `y_true` are\n",
       "`[batch_size, num_classes]`.\n",
       "\n",
       "Usage:\n",
       "\n",
       "```python\n",
       "cce = tf.keras.losses.CategoricalCrossentropy()\n",
       "loss = cce(\n",
       "  [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],\n",
       "  [[.9, .05, .05], [.05, .89, .06], [.05, .01, .94]])\n",
       "print('Loss: ', loss.numpy())  # Loss: 0.0945\n",
       "```\n",
       "\n",
       "Usage with the `compile` API:\n",
       "\n",
       "```python\n",
       "model = tf.keras.Model(inputs, outputs)\n",
       "model.compile('sgd', loss=tf.keras.losses.CategoricalCrossentropy())\n",
       "```\n",
       "\n",
       "Args:\n",
       "  from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n",
       "    we assume that `y_pred` encodes a probability distribution.\n",
       "    Note: Using from_logits=True may be more numerically stable.\n",
       "  label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n",
       "    meaning the confidence on label values are relaxed. e.g.\n",
       "    `label_smoothing=0.2` means that we will use a value of `0.1` for label\n",
       "    `0` and `0.9` for label `1`\"\n",
       "  reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.\n",
       "    Default value is `AUTO`. `AUTO` indicates that the reduction option will\n",
       "    be determined by the usage context. For almost all cases this defaults to\n",
       "    `SUM_OVER_BATCH_SIZE`.\n",
       "    When used with `tf.distribute.Strategy`, outside of built-in training\n",
       "    loops such as `tf.keras` `compile` and `fit`, using `AUTO` or\n",
       "    `SUM_OVER_BATCH_SIZE` will raise an error. Please see\n",
       "    https://www.tensorflow.org/tutorials/distribute/custom_training\n",
       "    for more details on this.\n",
       "  name: Optional name for the op.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CategoricalCrossentropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce = CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}_1$のクロスエントロピー損失は、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4377097"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cce(y_train_1,y_1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{Y}$のクロスエントロピー損失は、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3522456"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cce(Y_train_1_4,Y_1_4).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失を求められたので、次は勾配を求めます。実は、`keras`の関数である`gradient`を用いると`RuntimeError`がでます。\n",
    "そのため、今回は`tf.GradientTape()`を用いて求めます。\n",
    "ちなみに、先ほど定義したモデルから、weightの行列を取得し、そのweight行列と`numpy`の`gradient`を用いて計算して、gradientを求めようと思ったのですが、`numpy.gradient`の仕様がよくわからなくてやめました。誰か書ける人がいたら教えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad(x,y):\n",
    "    \n",
    "    def _loss_fn(y_predict, y):\n",
    "        return cce(y_predict, y)\n",
    "\n",
    "    inputs = tf.convert_to_tensor(x)\n",
    "    targets = tf.convert_to_tensor(y)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_predict = model(inputs)\n",
    "        loss = _loss_fn(y_predict, targets)\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    \n",
    "    return grads[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04970098, -0.01701283, -0.01685529, -0.01583286],\n",
       "       [-0.01669423,  0.0057145 ,  0.00566158,  0.00531815],\n",
       "       [ 0.01343682, -0.00459948, -0.00455688, -0.00428047],\n",
       "       ...,\n",
       "       [-0.1963863 ,  0.06722378,  0.06660128,  0.06256128],\n",
       "       [-0.15187682,  0.05198802,  0.0515066 ,  0.04838224],\n",
       "       [ 0.29698431, -0.10165887, -0.10071749, -0.09460802]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_grad(x_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 4)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_grad(x_1, y_train_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11603267,  0.04916372,  0.04986542,  0.01700349],\n",
       "       [-0.11608543,  0.05008772,  0.05015508,  0.01584258],\n",
       "       [ 0.02115218, -0.00044278, -0.00016743, -0.02054194],\n",
       "       ...,\n",
       "       [-0.08123177,  0.00707258,  0.00925068,  0.06490856],\n",
       "       [-0.16651726,  0.06467234,  0.0660722 ,  0.03577271],\n",
       "       [ 0.09841748, -0.00613205, -0.0088886 , -0.0833969 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_grad(X_1_4, Y_train_1_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_grad(X_1_4, Y_train_1_4).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列Wを学習せよ.\n",
    "\n",
    "とのことなので、さっきのモデルを用いて、行いたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10672 samples, validate on 1334 samples\n",
      "Epoch 1/100\n",
      "10672/10672 [==============================] - 1s 47us/sample - loss: 1.2072 - accuracy: 0.5678 - val_loss: 1.0972 - val_accuracy: 0.7166\n",
      "Epoch 2/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 1.0327 - accuracy: 0.7548 - val_loss: 0.9942 - val_accuracy: 0.7564\n",
      "Epoch 3/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.9508 - accuracy: 0.7716 - val_loss: 0.9278 - val_accuracy: 0.7564\n",
      "Epoch 4/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 0.8937 - accuracy: 0.7754 - val_loss: 0.8778 - val_accuracy: 0.7586\n",
      "Epoch 5/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.8495 - accuracy: 0.7760 - val_loss: 0.8379 - val_accuracy: 0.7601\n",
      "Epoch 6/100\n",
      "10672/10672 [==============================] - 0s 28us/sample - loss: 0.8139 - accuracy: 0.7768 - val_loss: 0.8052 - val_accuracy: 0.7624\n",
      "Epoch 7/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 0.7843 - accuracy: 0.7778 - val_loss: 0.7776 - val_accuracy: 0.7631\n",
      "Epoch 8/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 0.7591 - accuracy: 0.7787 - val_loss: 0.7539 - val_accuracy: 0.7631\n",
      "Epoch 9/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 0.7373 - accuracy: 0.7791 - val_loss: 0.7334 - val_accuracy: 0.7631\n",
      "Epoch 10/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 0.7182 - accuracy: 0.7795 - val_loss: 0.7151 - val_accuracy: 0.7631\n",
      "Epoch 11/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.7011 - accuracy: 0.7801 - val_loss: 0.6985 - val_accuracy: 0.7639\n",
      "Epoch 12/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.6859 - accuracy: 0.7810 - val_loss: 0.6837 - val_accuracy: 0.7646\n",
      "Epoch 13/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 0.6720 - accuracy: 0.7815 - val_loss: 0.6704 - val_accuracy: 0.7646\n",
      "Epoch 14/100\n",
      "10672/10672 [==============================] - 0s 25us/sample - loss: 0.6594 - accuracy: 0.7822 - val_loss: 0.6581 - val_accuracy: 0.7654\n",
      "Epoch 15/100\n",
      "10672/10672 [==============================] - 0s 25us/sample - loss: 0.6478 - accuracy: 0.7831 - val_loss: 0.6467 - val_accuracy: 0.7654\n",
      "Epoch 16/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 0.6370 - accuracy: 0.7835 - val_loss: 0.6362 - val_accuracy: 0.7684\n",
      "Epoch 17/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 0.6271 - accuracy: 0.7854 - val_loss: 0.6263 - val_accuracy: 0.7699\n",
      "Epoch 18/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.6178 - accuracy: 0.7864 - val_loss: 0.6172 - val_accuracy: 0.7736\n",
      "Epoch 19/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.6091 - accuracy: 0.7882 - val_loss: 0.6086 - val_accuracy: 0.7774\n",
      "Epoch 20/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.6009 - accuracy: 0.7905 - val_loss: 0.6004 - val_accuracy: 0.7796\n",
      "Epoch 21/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.5932 - accuracy: 0.7921 - val_loss: 0.5927 - val_accuracy: 0.7841\n",
      "Epoch 22/100\n",
      "10672/10672 [==============================] - 0s 28us/sample - loss: 0.5859 - accuracy: 0.7944 - val_loss: 0.5853 - val_accuracy: 0.7849\n",
      "Epoch 23/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 0.5790 - accuracy: 0.7965 - val_loss: 0.5784 - val_accuracy: 0.7894\n",
      "Epoch 24/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.5725 - accuracy: 0.7998 - val_loss: 0.5720 - val_accuracy: 0.7916\n",
      "Epoch 25/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.5662 - accuracy: 0.8019 - val_loss: 0.5655 - val_accuracy: 0.7969\n",
      "Epoch 26/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.5603 - accuracy: 0.8052 - val_loss: 0.5596 - val_accuracy: 0.7999\n",
      "Epoch 27/100\n",
      "10672/10672 [==============================] - 0s 32us/sample - loss: 0.5546 - accuracy: 0.8070 - val_loss: 0.5539 - val_accuracy: 0.8036\n",
      "Epoch 28/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 0.5492 - accuracy: 0.8089 - val_loss: 0.5484 - val_accuracy: 0.8073\n",
      "Epoch 29/100\n",
      "10672/10672 [==============================] - 0s 27us/sample - loss: 0.5440 - accuracy: 0.8103 - val_loss: 0.5431 - val_accuracy: 0.8088\n",
      "Epoch 30/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.5391 - accuracy: 0.8121 - val_loss: 0.5381 - val_accuracy: 0.8118\n",
      "Epoch 31/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.5343 - accuracy: 0.8142 - val_loss: 0.5332 - val_accuracy: 0.8156\n",
      "Epoch 32/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.5297 - accuracy: 0.8162 - val_loss: 0.5286 - val_accuracy: 0.8178\n",
      "Epoch 33/100\n",
      "10672/10672 [==============================] - 0s 26us/sample - loss: 0.5253 - accuracy: 0.8182 - val_loss: 0.5242 - val_accuracy: 0.8193\n",
      "Epoch 34/100\n",
      " 2272/10672 [=====>........................] - ETA: 0s - loss: 0.5302 - accuracy: 0.8217"
     ]
    }
   ],
   "source": [
    "opt = SGD(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs=100, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74.正解率の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss:{0}'.format(score[0]))\n",
    "print('Test accuracy:{0}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
